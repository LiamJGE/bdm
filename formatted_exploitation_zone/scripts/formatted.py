import pyspark
import json
import os
from pyspark.sql import Row
from pymongo import MongoClient
from pyspark.sql import SparkSession

from tqdm import tqdm
import logging


IP = '10.4.41.42'
USER = 'bdm'
PWD = 'bdm'
PORT = '27017'
URI = f"mongodb://{PWD}:{USER}@{IP}:{PORT}"
FORMATTED_DB = 'formatted_zone'


# Setting up logger
logs_dir = '../logs/'
if not os.path.exists(logs_dir):
        os.makedirs(logs_dir)
if not os.path.exists(os.path.join(logs_dir, "processedElements.log")):
    file = open(os.path.join(logs_dir, "processedElements.log"), "a")
    file.close()
logging.basicConfig(filename='../logs/processedElements.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')

# Configure Spark
sc = pyspark.SparkContext.getOrCreate()
ss = SparkSession.builder.getOrCreate()

def read_parquet_data(path):
    return ss.read.parquet(path).rdd

def read_json_data(path):
    return sc.textFile(path).map(json.loads)

# Read Idealista data from all Parquet files in subdirectories
def read_all_parquet_files(root_path):
    rdds = []
    print(f"Searching in root_path: {root_path}")
    for subdir, _, files in os.walk(root_path):
        for file in files:
            if file.endswith(".parquet"):
                file_path = os.path.join(subdir, file)
                rdds.append(read_parquet_data(file_path))
    return sc.union(rdds)

# Read data from sources
def read_data(sources):
    data = {}
    for source_name, source_config in sources.items():
        if source_config["type"] == "parquet":
            data[source_name] = read_all_parquet_files(source_config["path"])
        elif source_config["type"] == "json":
            data[source_name] = read_json_data(source_config["path"])
    return data

# Converts all RDD Rows to Dict. It is a recursive function so it covers nested rows
def row_to_dict(row):
    d = row.asDict()
    for k in d.keys():
        if isinstance(d[k], Row):
            d[k] = row_to_dict(d[k])
    return d

# Remove ical attribute because it does not add anything except space
def remove_ical(dictionary):
    if 'ical' in dictionary:
        del dictionary['ical']
    return dictionary

# Integration and reconciliation function
def integrate_and_reconciliate(data):
    # Clean data from sources
    idealista_data = data["idealista"].distinct()
    # Cannot apply distinct because dict objects are unhashable and therefore cannot be compared
    territorial_distribution_data = data["territorial_distribution"]
    # We apply flatMap due to the original format of data
    musica_copes_data = data["musica_i_copes"].flatMap(lambda x: x)

    # Remove useless attribute (long URL)
    musica_copes_data = musica_copes_data.map(remove_ical)

    # Create mappings for rent_lookup_district and rent_lookup_neighborhood
    rent_lookup_district_mapping = data["rent_lookup_district"].keyBy(lambda row: row['di'])
    
    # Create mappings for rent_lookup_neighborhood
    rent_lookup_neighborhood_mapping = data["rent_lookup_neighborhood"].keyBy(lambda row: row["ne"])
    
    # Join with district mapping
    idealista_data = idealista_data \
        .map(row_to_dict) \
        .keyBy(lambda x: str(x['district'])) \
        .leftOuterJoin(rent_lookup_district_mapping) \
        .mapValues(lambda x: {**x[0], "district_id": x[1]["_id"] if x[1] else None})
    
    # Join with neighborhood mapping
    idealista_data = idealista_data \
        .flatMap(lambda x: [(x[1]['neighborhood'], x[1])] if 'neighborhood' in x[1] else [(None, x[1])]) \
        .leftOuterJoin(rent_lookup_neighborhood_mapping) \
        .mapValues(lambda x: {**x[0], "neighborhood_id": x[1]["_id"] if x[1] else None})
    
    
    # Create mappings for income_lookup_district
    income_lookup_district_mapping = data["income_lookup_district"].keyBy(lambda row: row["district"])
    
    # Create mappings for income_lookup_neighborhood
    income_lookup_neighborhood_mapping = data["income_lookup_neighborhood"].keyBy(lambda row: row["neighborhood"])
    
    # Join with income_lookup_district_mapping
    territorial_distribution_data = territorial_distribution_data \
        .keyBy(lambda x: str(x['district_name'])) \
        .leftOuterJoin(income_lookup_district_mapping) \
        .mapValues(lambda x: {**x[0], "district_id": x[1]["_id"] if x[1] else None})

    # Join with income_lookup_neighborhood_mapping
    territorial_distribution_data = territorial_distribution_data \
        .flatMap(lambda x: [(x[1]['neigh_name '], x[1])] if 'neigh_name ' in x[1].keys() else [(None, x[1])]) \
        .leftOuterJoin(income_lookup_neighborhood_mapping) \
        .mapValues(lambda x: {**x[0], "neighborhood_id": x[1]["_id"] if x[1] else None})

    # Join with income_lookup_district_mapping
    musica_copes_data = musica_copes_data \
        .keyBy(lambda x: str(x['addresses'][0]['district_name'])) \
        .leftOuterJoin(income_lookup_district_mapping) \
        .mapValues(lambda x: {**x[0], "district_id": x[1]["_id"] if x[1] else None})

    # Join with income_lookup_neighborhood_mapping
    musica_copes_data = musica_copes_data \
        .flatMap(lambda x: [(x[1]['addresses'][0]['neighborhood_name'], x[1])] if 'addresses' in x[1].keys() and 'neighborhood_name' in x[1]['addresses'][0].keys() else [(None, x[1])]) \
        .leftOuterJoin(income_lookup_neighborhood_mapping) \
        .mapValues(lambda x: {**x[0], "neighborhood_id": x[1]["_id"] if x[1] else None})
    
    # Store results in a dictionary with the corresponding data source name
    result = {
        "idealista": idealista_data,
        "territorial_distribution": territorial_distribution_data,
        "musica_i_copes": musica_copes_data
    }

    return result

def save_formatted_data(uri, database, collection, data):
    client = MongoClient(uri)
    db = client[database]
    documents = []
    for t in data:
        if type(t) == tuple:
            r = t[1]
        else:
            r = t
        if '_id' in r.keys():
            db[collection].find_one_and_delete({'_id': r['_id']})
            logging.info(f'Replacing id {r["_id"]} in collection {collection}')
        documents.append(r)
    
    logging.info(f'Processed {len(documents)} elements for {collection} collection')

    db[collection].insert_many(documents)

# Data sources configuration
data_sources = {
    "idealista": {
        "type": "parquet",
        "path": "../P2_data/idealista"
    },
    "territorial_distribution": {
        "type": "json",
        "path": "../P2_data/income_opendata/income_opendata_neighborhood.json"
    },
     "rent_lookup_district": {
        "type": "json",
        "path": "../P2_data/lookup_tables/rent_lookup_district.json"
    },
    "rent_lookup_neighborhood": {
        "type": "json",
        "path": "../P2_data/lookup_tables/rent_lookup_neighborhood.json"
    }, 
    "income_lookup_district": {
        "type": "json",
        "path": "../P2_data/lookup_tables/income_lookup_district.json"
    },
    "income_lookup_neighborhood": {
        "type": "json",
        "path": "../P2_data/lookup_tables/income_lookup_neighborhood.json"
    },
    "musica_i_copes": {
        "type": "json",
        "path": "../P2_data/third_dataset/opendatabcn_cultura_espais-de-musica-i-copes-js.json"
    }
    # ... add more data sources if needed
}

# Perform data integration and reconciliation
raw_data = read_data(data_sources)

print('Performing integration and reconciliation processes\n')

formatted_data_dict  = integrate_and_reconciliate(raw_data)

# Save the formatted data to MongoDB
print('Uploading to MongoDB')
for source_name, source_config in tqdm(data_sources.items()):
    if source_name in ["idealista", "territorial_distribution", "musica_i_copes"] :
        formatted_data = formatted_data_dict[source_name]
    else:
        formatted_data = raw_data[source_name]
    save_formatted_data(URI, FORMATTED_DB, source_name, formatted_data.collect())
print('\nUploaded!')

ss.stop()
