from pyspark.sql import SparkSession
from pyspark.sql import Row
from pymongo import MongoClient
import csv
from hdfs import InsecureClient
import os
import logging

IP = '10.4.41.42'
USER = 'bdm'
PWD = 'bdm'
PORT = '27017'
URI = f"mongodb://{PWD}:{USER}@{IP}:{PORT}"
FORMATTED_DB = 'formatted_zone'
EXPLOITATION_DB = 'exploitation_zone'

# Setting up logger
logs_dir = 'logs/'
if not os.path.exists(logs_dir):
        os.makedirs(logs_dir)
if not os.path.exists(os.path.join(logs_dir, "processedElements.log")):
    file = open(os.path.join(logs_dir, "processedElements.log"), "a")
    file.close()
logging.basicConfig(filename='../logs/processedInstances.log', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')


def connect_database(ip, db):
    # Create spark session
    s = SparkSession \
        .builder \
        .appName("myApp") \
        .config("spark.mongodb.input.uri", f"mongodb://{ip}/{db}") \
        .config("spark.mongodb.output.uri", f"mongodb://{ip}/{db}") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.skewJoin.enabled", "true") \
        .config("spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.12:3.0.1") \
        .getOrCreate()
    return s

def save_to_mongodb(data, ip, db, collection):
    data.write.format("mongo").option("uri", f"mongodb://{ip}/{db}.{collection}").mode("append").save()

# Converts all Row objects to dict, including nested Rows
def row_to_dict(row):
    d = row.asDict()
    for k in d.keys():
        if isinstance(d[k], Row):
            d[k] = row_to_dict(d[k])
    return d

# Flattens dictionary to be able to create csv
def flatten_dict(dictionary, parent_key='', sep='_'):
    items = []
    for key, value in dictionary.items():
        new_key = f"{parent_key}{sep}{key}" if parent_key else key
        if isinstance(value, dict):
            items.extend(flatten_dict(value, new_key, sep=sep).items())
        else:
            items.append((new_key, value))
    return dict(items)

# We calculate averages of income data attributes
def aggregate_income(data):
    data = data.flatMap(lambda x: [(x['neighborhood_id'], (item['RFD'], item['pop'], 1)) for item in x['info']]) \
             .reduceByKey(lambda a, b: (a[0] + b[0], a[1] + b[1], a[2] + b[2])) \
             .map(lambda x: (x[0], {'RFD': round(x[1][0] / x[1][2], ndigits=2), 'pop': round(x[1][1] / x[1][2], ndigits=2)})) \
    
    return data

# Save data to MongoDB
def save_data_mongo(uri, database, collection, data):
    client = MongoClient(uri)
    db = client[database]
    documents = []
    for t in data:
        if type(t) == tuple:
            r = t[1]
        else:
            r = t
        if '_id' in r.keys():
            db[collection].find_one_and_delete({'_id': r['_id']})
            logging.info(f'Replacing id {r["_id"]} in collection {collection}')
        documents.append(r)
    
    logging.info(f'Processed {len(documents)} elements for {collection} collection')

    db[collection].insert_many(documents)


# Correct establishment names by removing .
def correct_establishment_name(name):
    return name.replace('.', '')

# Saves the collected RDD to a csv in local machine
def to_csv(data, keys, filename='idealista_income_musica_i_copes.csv'):
    with open(filename, 'w', newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=keys)
        writer.writeheader()

        # Write each dictionary as a row in the CSV file
        for d in data:
            writer.writerow(d[1])

# Uploads local csv to hdfs
def save_hdfs(hdfs_path='/user/bdm', local_path='../aux_data/idealista_income_musica_i_copes.csv'):
    hdfs_con = InsecureClient(f'http://{IP}:9870', user='bdm')
    
    # Upload the CSV file to HDFS
    hdfs_con.upload(hdfs_path, local_path, overwrite=True)

# Joins all three sources
def join_idealista_income_musica(ip, db, s):
    # Load data from MongoDB from formatted_zone
    idealista = sparkFormatted.read.format("mongo").option("uri", f"mongodb://{ip}/{db}.idealista").load().rdd
    income = sparkFormatted.read.format("mongo").option("uri", f"mongodb://{ip}/{db}.territorial_distribution").load().rdd
    musica_i_copes = sparkFormatted.read.format("mongo").option("uri", f"mongodb://{ip}/{db}.musica_i_copes").load().rdd
    
    print("Data loaded from MongoDB\n")


    print("Idealista count: ", idealista.count())

    print("Income count: ", income.count())
    
    print("musica_i_copes count: ", musica_i_copes.count())

    # Extract all unique establishment types
    establishments_types = musica_i_copes.flatMap(lambda x: [correct_establishment_name(item['name']).strip() for item in x['secondary_filters_data']]).distinct().collect()


    # Filter out records with None neighborhood_id
    idealista = idealista.filter(lambda row: row.neighborhood_id is not None) \
                .map(lambda row: Row(**{k: v for k, v in row.asDict().items() if k != '_id'})) \
                .map(row_to_dict)
    income = income.filter(lambda row: row.neighborhood_id is not None)
    musica_i_copes = musica_i_copes.filter(lambda row: row.neighborhood_id is not None)

    # Convert to PairRDDs using neighborhood_id as the key
    idealista_key = idealista.keyBy(lambda row: row['neighborhood_id'])
    income_key = aggregate_income(data=income)

    musica_i_copes_key = musica_i_copes.flatMap(lambda x: [(x['neighborhood_id'], {correct_establishment_name(item['name']).strip(): 1 for item in x['secondary_filters_data']})]) \
        .reduceByKey(lambda a, b: {k: a.get(k, 0) + b.get(k, 0) for k in establishments_types})
    
    # Filter out establishment types with count 0 in all rows
    musica_i_copes_key = musica_i_copes_key.mapValues(lambda x: {k: v for k, v in x.items() if v != 0})

    # Continue with existing transformation
    musica_i_copes_key = musica_i_copes_key.map(lambda x: (x[0], {"establishments": x[1]}))
    
    # Join RDDs
    kpi1 = musica_i_copes_key.join(income_key)
    kpi1 = kpi1.map(lambda x: (x[0], {**x[1][0], **x[1][1], 'neighborhood_id':x[0]}))
    
    # Repartition due to data skew
    idealista_key = idealista_key.repartition(4)
    kpi2 = idealista_key.join(kpi1)
    
    # Flatten the result
    kpi2 = kpi2.map(lambda x: (x[0], {**x[1][0], **x[1][1]})) \
           .map(lambda x: (x[0], flatten_dict(x[1])))
    
    print('Saving data to Mongo\n')
    save_data_mongo(uri=URI, database=EXPLOITATION_DB, collection='idealista_income_musica_i_copes', data=kpi2.collect())

    # We create a list of all the keys we can have
    keys = kpi2.flatMap(lambda row: row[1].keys()).distinct().collect()
    # Fill missing keys with default value (None)
    kpi2_filled = kpi2.map(lambda row: (row[0], {key: row[1].get(key, None) for key in keys}))

    # Export collected RDD to csv
    print('Exporting to CSV\n')
    to_csv(data=kpi2_filled.collect(), keys=keys, filename='../aux_data/idealista_income_musica_i_copes.csv')

    print('Saved CSV\n')

    print('Uploading to HDFS\n')
    save_hdfs()

    print('Saved to HDFS\n')

sparkFormatted = connect_database(ip=IP, db=FORMATTED_DB)
join_idealista_income_musica(ip=IP, db=FORMATTED_DB, s=sparkFormatted)

# Stop session to avoid TimeoutException
sparkFormatted.stop()